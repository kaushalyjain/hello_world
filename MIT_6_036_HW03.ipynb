{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MIT 6.036 HW03",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kaushalyjain/hello_world/blob/master/MIT_6_036_HW03.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xIaEwCD406A",
        "colab_type": "text"
      },
      "source": [
        "#MIT 6.036 Fall 2019: Homework 3#\n",
        "\n",
        "This colab notebook provides code and a framework for problems 1-7 of the \n",
        "[ homework](https://lms.mitx.mit.edu/courses/course-v1:MITx+6.036+2019_Fall/courseware/Week3/week3_homework/).  You can work out your solutions here, then submit your results back on the homework page when ready.\n",
        "\n",
        "## <section>**Setup**</section>\n",
        "\n",
        "First, download the code distribution for this homework that contains test cases and helper functions.\n",
        "\n",
        "Run the next code block to download and import the code for this lab.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2YM-_zLf9Bp-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 580
        },
        "outputId": "47a0be6c-5289-44ad-8981-8851f7b08de9"
      },
      "source": [
        "!rm -rf code_and_data_for_hw3*\n",
        "!rm -rf mnist\n",
        "!wget --quiet https://introml.odl.mit.edu/cat-soop/_static/6.036/homework/hw03/code_and_data_for_hw3.zip\n",
        "!unzip code_and_data_for_hw3.zip\n",
        "!mv code_and_data_for_hw3/* .\n",
        "  \n",
        "from code_for_hw3_part1 import *\n",
        "import code_for_hw3_part2 as hw3\n",
        "import numpy as np"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  code_and_data_for_hw3.zip\n",
            "   creating: code_and_data_for_hw3/\n",
            "   creating: code_and_data_for_hw3/mnist/\n",
            "  inflating: code_and_data_for_hw3/mnist/mnist_train4.png  \n",
            "  inflating: code_and_data_for_hw3/mnist/mnist_train5.png  \n",
            "  inflating: code_and_data_for_hw3/mnist/mnist_train7.png  \n",
            "  inflating: code_and_data_for_hw3/mnist/mnist_train6.png  \n",
            "  inflating: code_and_data_for_hw3/mnist/mnist_train2.png  \n",
            "  inflating: code_and_data_for_hw3/mnist/mnist_train3.png  \n",
            "  inflating: code_and_data_for_hw3/mnist/mnist_train1.png  \n",
            "  inflating: code_and_data_for_hw3/mnist/mnist_train0.png  \n",
            "  inflating: code_and_data_for_hw3/mnist/mnist_train8.png  \n",
            "  inflating: code_and_data_for_hw3/mnist/mnist_train9.png  \n",
            "  inflating: code_and_data_for_hw3/code_for_hw3_part2.py  \n",
            "  inflating: code_and_data_for_hw3/auto-mpg.tsv  \n",
            "  inflating: code_and_data_for_hw3/code_for_hw3_part1.py  \n",
            "  inflating: code_and_data_for_hw3/reviews.tsv  \n",
            "  inflating: code_and_data_for_hw3/stopwords.txt  \n",
            "  inflating: code_and_data_for_hw3/hw3_part2_main.py  \n",
            "Importing code_for_hw03\n",
            "Imported tidy_plot, plot_separator, plot_data, plot_nonlin_sep, cv, rv, y, positive, score\n",
            "Datasets: super_simple_separable_through_origin(), super_simple_separable(), xor(), xor_more()\n",
            "Tests for part 2: test_linear_classifier_with_features, mul, make_polynomial_feature_fun, \n",
            "                  test_with_features\n",
            "Also loaded: perceptron, one_hot_internal, test_one_hot\n",
            "Importing code_for_hw03 (part 2, imported as hw3)\n",
            "Imported tidy_plot, plot_separator, plot_data, plot_nonlin_sep, cv, rv, y, positive, score\n",
            "         xval_learning_alg, eval_classifier\n",
            "Tests: test_linear_classifier\n",
            "Dataset tools: load_auto_data, std_vals, standard, raw, one_hot, auto_data_and_labels\n",
            "               load_review_data, clean, extract_words, bag_of_words, extract_bow_feature_vectors\n",
            "               load_mnist_data, load_mnist_single\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xFxhrJ5XDlvb",
        "colab_type": "text"
      },
      "source": [
        "# Feature Transformation\n",
        "\n",
        "## <section>**Running Perceptron**</section>\n",
        "\n",
        "In problems 1,2 and 3, you will have to run the Perceptron algorithm several times to obtain linear classifiers.\n",
        "We provide you with an implementation of the algorithm which you can use to obtain your results.\n",
        "\n",
        "The specifications for the `perceptron` method provided are:\n",
        "* `data` is a numpy array of dimension $d$ by $n$\n",
        "* `labels` is numpy array of dimension $1$ by $n$\n",
        "* `params` is a dictionary specifying extra parameters to this algorithm; your algorithm runs a number of iterations equal to $T$\n",
        "* `hook` is either None or a function that takes the tuple `(th, th0)` as an argument and displays the separator graphically. \n",
        "\n",
        "It should return a tuple of $\\theta$ (a $d$ by 1 array) and $\\theta_0$ (a 1 by 1 array).\n",
        "\n",
        "Note that you are free to modify the method. For example, a useful modification for this homework would be to make the method return the number of mistakes made on the input data, while it runs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VtYf8ysk-VQU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Perceptron algorithm with offset.\n",
        "# data is dimension d by n\n",
        "# labels is dimension 1 by n\n",
        "# T is a positive integer number of steps to run\n",
        "def perceptron(data, labels, params = {}, hook = None):\n",
        "    # if T not in params, default to 50\n",
        "    T = params.get('T', 10000)\n",
        "    (d, n) = data.shape\n",
        "    mistakes = 0\n",
        "    theta = np.zeros((d, 1)); theta_0 = np.zeros((1, 1))\n",
        "    for t in range(T):\n",
        "        for i in range(n):\n",
        "            x = data[:,i:i+1]\n",
        "            y = labels[:,i:i+1]\n",
        "            if y * positive(x, theta, theta_0) <= 0.0:\n",
        "                mistakes += 1\n",
        "                theta = theta + y * x\n",
        "                theta_0 = theta_0 + y\n",
        "                if hook: hook((theta, theta_0))\n",
        "    return theta, theta_0\n",
        "\n",
        "def averaged_perceptron(data, labels, params = {}, hook = None):\n",
        "    T = params.get('T', 50)\n",
        "    (d, n) = data.shape\n",
        "\n",
        "    theta = np.zeros((d, 1)); theta_0 = np.zeros((1, 1))\n",
        "    theta_sum = theta.copy()\n",
        "    theta_0_sum = theta_0.copy()\n",
        "    for t in range(T):\n",
        "        for i in range(n):\n",
        "            x = data[:,i:i+1]\n",
        "            y = labels[:,i:i+1]\n",
        "            if y * positive(x, theta, theta_0) <= 0.0:\n",
        "                theta = theta + y * x\n",
        "                theta_0 = theta_0 + y\n",
        "                if hook: hook((theta, theta_0))\n",
        "            theta_sum = theta_sum + theta\n",
        "            theta_0_sum = theta_0_sum + theta_0\n",
        "    theta_avg = theta_sum / (T*n)\n",
        "    theta_0_avg = theta_0_sum / (T*n)\n",
        "    if hook: hook((theta_avg, theta_0_avg))\n",
        "    return theta_avg, theta_0_avg\n",
        "\n",
        "  \n",
        "def eval_classifier(learner, data_train, labels_train, data_test, labels_test):\n",
        "    th, th0 = learner(data_train, labels_train)\n",
        "    return score(data_test, labels_test, th, th0)/data_test.shape[1]\n",
        "\n",
        "def positive(x, th, th0):\n",
        "    return np.sign(th.T@x + th0)\n",
        "\n",
        "def score(data, labels, th, th0):\n",
        "    return np.sum(positive(data, th, th0) == labels)\n",
        "\n",
        "def xval_learning_alg(learner, data, labels, k):\n",
        "    _, n = data.shape\n",
        "    idx = list(range(n))\n",
        "    np.random.seed(0)\n",
        "    np.random.shuffle(idx)\n",
        "    data, labels = data[:,idx], labels[:,idx]\n",
        "\n",
        "    score_sum = 0\n",
        "    s_data = np.array_split(data, k, axis=1)\n",
        "    s_labels = np.array_split(labels, k, axis=1)\n",
        "    for i in range(k):\n",
        "        data_train = np.concatenate(s_data[:i] + s_data[i+1:], axis=1)\n",
        "        labels_train = np.concatenate(s_labels[:i] + s_labels[i+1:], axis=1)\n",
        "        data_test = np.array(s_data[i])\n",
        "        labels_test = np.array(s_labels[i])\n",
        "        score_sum += eval_classifier(learner, data_train, labels_train,\n",
        "                                              data_test, labels_test)\n",
        "    return score_sum/k\n",
        "  \n",
        "  \n",
        "def one_hot(x, k):\n",
        "    onehot = np.zeros((k,1))\n",
        "    onehot[x-1,0] = 1\n",
        "    return onehot\n",
        "    pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "53YDYZo38ktq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = np.array([[0.200, 0.800, 0.200, 0.800],\n",
        "                 [0.2,  0.2,  0.8,  0.8]])\n",
        "labels = np.array([[-1, -1, 1, 1]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3g96gm5hmQja",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "c9c9bc43-88f7-4949-92e7-86b562c4b964"
      },
      "source": [
        "data =   np.concatenate((one_hot(1,6), one_hot(2,6), one_hot(3,6), one_hot(4,6), one_hot(5,6), one_hot(6,6)),axis=1)\n",
        "labels = np.array([[1, 1, -1, -1,1,1]])\n",
        "\n",
        "perceptron(data, labels, params = {'T':100}, hook = None)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[ 1.],\n",
              "        [ 1.],\n",
              "        [-2.],\n",
              "        [-2.],\n",
              "        [ 1.],\n",
              "        [ 1.]]), array([[0.]]), 8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZzqUR755Joij",
        "colab_type": "text"
      },
      "source": [
        "## <section>2D) Encoding Discrete Values</section>\n",
        "\n",
        "It is common to encode sets of discrete values, for machine learning, not as a single multi-valued feature, but using a one hot encoding. So, if there are $k$ values in the discrete set, we would transform that single multi-valued feature into $k$ binary-valued features, in which feature $i$ has value $+1$ if the original feature value was $i$ and has value $0$ (or $-1$) otherwise.\n",
        "\n",
        "Write a function `one_hot` that takes as input $x$, a single feature value (between $1$ and $k$), and $k$, the total possible number of values this feature can take on, and transform it to a numpy column vector of $k$ binary features using a one-hot encoding (remember vectors have zero-based indexing)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R11VPNJ3Jpwb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def one_hot(x, k):\n",
        "    pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uOdWFevZJs1U",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "19d0f359-13d6-4a9b-9145-b0b3c79288dd"
      },
      "source": [
        "test_one_hot(one_hot)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Passed! \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IHCfY1dBJvK3",
        "colab_type": "text"
      },
      "source": [
        "## 3) Polynomial Features\n",
        "\n",
        "One systematic way of generating non-linear transformations of your input features is to consider the polynomials of increasing order.  Given a feature vector $x = [x_1, x_2, ..., x_d]^T$, we can map it into a new feature vector that contains all the factors in a polynomial of order $d$. For example, for $x = [x_1, x_2]^T$ and order 2, we get $$\\phi(x) = [1, x_1, x_2, x_1x_2, x_1^2, x_2^2]^T$$ and for order 3, we get $$\\phi(x) = [1, x_1, x_2, x_1x_2, x_1^2, x_2^2, x_1^2x_2, x_1x_2^2, x_1^3, x_2^3]^T.$$  \n",
        "\n",
        "In the code that has been loaded, we have defined `make_polynomial_feature_fun` that, given the order, returns a feature transformation function (analogous to $\\phi$ in the description).  You should use it in doing this problem."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2MHF3Ej7Jx0r",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "c870be3c-e510-455f-b7a2-bc8dbe7c36c4"
      },
      "source": [
        "## For example, make_polynomial_feature_fun could be used as follows:\n",
        "\n",
        "# Generate transformation of order 2\n",
        "\n",
        "for i in [1, 10, 20, 30, 40, 50]:\n",
        "  transformation = make_polynomial_feature_fun(i)\n",
        "\n",
        "  # Data\n",
        "  data = np.zeros((2,1))\n",
        "\n",
        "  # Use transformation on data\n",
        "  print(i, transformation(data).size)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1 3\n",
            "10 66\n",
            "20 231\n",
            "30 496\n",
            "40 861\n",
            "50 1326\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gwVEBQ_dMoxM",
        "colab_type": "text"
      },
      "source": [
        "Note that iterative animations, which update a plot within a loop, don't work the same way in colab, as with a local python console installation.  One workaround for colab to be able to show such plot iterations is to show all the plots, and this can be done for the test code using this patched function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U87UfhraMn7P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test_linear_classifier_with_features(dataFun, learner, feature_fun,\n",
        "                             draw = True, refresh = True, pause = True):\n",
        "    raw_data, labels = dataFun()\n",
        "    data = feature_fun(raw_data) if feature_fun else raw_data\n",
        "    if draw:\n",
        "        def hook(params):\n",
        "            ax = plot_data(raw_data, labels)   # create plot axis on each iteration\n",
        "            (th, th0) = params\n",
        "            predictor = lambda x1,x2: int(positive(feature_fun(cv([x1, x2])), th, th0))\n",
        "            plot_nonlin_sep(\n",
        "                predictor,\n",
        "                ax = ax)\n",
        "            plot_data(raw_data, labels, ax)\n",
        "            plt.show()                         # force plot to push to the colab notebook and be displayed\n",
        "            print('th', th.T, 'th0', th0)\n",
        "            if pause: input('press enter here to continue:')\n",
        "    else:\n",
        "        hook = None\n",
        "    th, th0 = learner(data, labels, hook = hook)\n",
        "    if hook: hook((th, th0))\n",
        "    print(\"Final score\", int(score(data, labels, th, th0)))\n",
        "    print(\"Params\", np.transpose(th), th0)\n",
        "\n",
        "def test_with_features(dataFun, order = 2, draw=True, pause=True):\n",
        "    test_linear_classifier_with_features(\n",
        "        dataFun,                        # data\n",
        "        perceptron,                     # learner\n",
        "        make_polynomial_feature_fun(order), # feature maker\n",
        "        draw=draw,\n",
        "        pause=pause)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N7SLAtCuNSOq",
        "colab_type": "text"
      },
      "source": [
        "Here's a test you can run to see plots:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H74UrlZSNVe-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "6122a3ad-2717-4280-8d85-bd8aa6ff1c99"
      },
      "source": [
        "test_with_features(xor_more, order=3, draw=False, pause=False)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Final score 8\n",
            "Params [[ -78.   28.  -39.   72.  248.  -19.   76. -522.  476. -153.]] [[-78.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GcgPe4-XQ8-b",
        "colab_type": "text"
      },
      "source": [
        "# Experiments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "erNybvgGRXCf",
        "colab_type": "text"
      },
      "source": [
        "## 4) Car Data - Evaluating Algorithmic and Feature Choices\n",
        "\n",
        "We now want to build a classifier for the auto data, focusing on the\n",
        "numeric data.  In the code file for this part of the assignment, we have supplied you\n",
        "with the `load_auto_data` function, that can be used to read the\n",
        "relevant .tsv file.  It will return a list of dictionaries, one for each data item.\n",
        "\n",
        "We then have to specify what feature function to use for each column\n",
        "in the data.  The file `hw3_part2_main.py` has an example for constructing\n",
        "the data and label arrays using `raw` feature function for all the columns.\n",
        "Look at the definition of `features` in `hw3_part2_main.py`, this indicates a feature name to\n",
        "use and then a feature function, there are three defined in the\n",
        "`code_for_hw3_part2.py` file (`raw`, `standard` and `one_hot`).  `raw` just uses\n",
        "the original value, `standard` subtracts out the mean value and\n",
        "divides by the standard deviation and `one_hot` does the encoding\n",
        "described in the notes.\n",
        "\n",
        "The function `auto_data_and_labels` will process the dictionaries and\n",
        "return <tt>data, labels</tt> where <tt>data</tt> are arrays of\n",
        "dimension $(d, 392)$, with $d$ the total number of features specified,\n",
        "and <tt>labels</tt> is of dimension $(1, 392)$.  The data in the file\n",
        "is sorted by class, but it will be shuffled when you read it in."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3jdynxZqQ5Ky",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "0d979e44-0813-43a3-bc46-1d75b79eb53a"
      },
      "source": [
        "# Returns a list of dictionaries.  Keys are the column names, including mpg.\n",
        "auto_data_all = hw3.load_auto_data('auto-mpg.tsv')\n",
        "\n",
        "# The choice of feature processing for each feature, mpg is always raw and\n",
        "# does not need to be specified.  Other choices are hw3.standard and hw3.one_hot.\n",
        "# 'name' is not numeric and would need a different encoding.\n",
        "features = [('cylinders', hw3.raw),\n",
        "            ('displacement', hw3.raw),\n",
        "            ('horsepower', hw3.raw),\n",
        "            ('weight', hw3.raw),\n",
        "            ('acceleration', hw3.raw),\n",
        "            ## Drop model_year by default\n",
        "            ## ('model_year', hw3.raw),\n",
        "            ('origin', hw3.raw)]\n",
        "\n",
        "# Construct the standard data and label arrays\n",
        "auto_data, auto_labels = hw3.auto_data_and_labels(auto_data_all, features)\n",
        "print('auto data and labels shape', auto_data.shape, auto_labels.shape)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "avg and std {}\n",
            "entries in one_hot field {}\n",
            "auto data and labels shape (6, 392) (1, 392)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yfOrmU1XdFCZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 670
        },
        "outputId": "403bab06-9ce2-4b1c-dd5c-65b1bddd981d"
      },
      "source": [
        "# Your code here to process the auto data\n",
        "features2 = [('cylinders', hw3.one_hot),\n",
        "            ('displacement', hw3.standard),\n",
        "            ('horsepower', hw3.standard),\n",
        "            ('weight', hw3.standard),\n",
        "            ('acceleration', hw3.standard),\n",
        "            ## Drop model_year by default\n",
        "            ## ('model_year', hw3.raw),\n",
        "            ('origin', hw3.one_hot)]\n",
        "\n",
        "\n",
        "learner = perceptron\n",
        "for featureset in [features, features2]:\n",
        "  for tval in [1,10,50]:\n",
        "    for learner in [perceptron, averaged_perceptron]:\n",
        "      auto_data, auto_labels = hw3.auto_data_and_labels(auto_data_all, featureset)\n",
        "      print (tval, learner, xval_learning_alg(lambda auto_data, auto_labels : learner(auto_data, auto_labels, {\"T\": tval}), auto_data, auto_labels, k=10))\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "avg and std {}\n",
            "entries in one_hot field {}\n",
            "1 <function perceptron at 0x7fc41b8ce510> 0.6526282051282052\n",
            "avg and std {}\n",
            "entries in one_hot field {}\n",
            "1 <function averaged_perceptron at 0x7fc41b8ce730> 0.8441025641025641\n",
            "avg and std {}\n",
            "entries in one_hot field {}\n",
            "10 <function perceptron at 0x7fc41b8ce510> 0.7423076923076924\n",
            "avg and std {}\n",
            "entries in one_hot field {}\n",
            "10 <function averaged_perceptron at 0x7fc41b8ce730> 0.8366025641025641\n",
            "avg and std {}\n",
            "entries in one_hot field {}\n",
            "50 <function perceptron at 0x7fc41b8ce510> 0.6909615384615384\n",
            "avg and std {}\n",
            "entries in one_hot field {}\n",
            "50 <function averaged_perceptron at 0x7fc41b8ce730> 0.8366025641025641\n",
            "avg and std {'displacement': (388.3482142857143, 302.0458123396403), 'horsepower': (509.3545918367347, 333.6521151716361), 'weight': (2977.5841836734694, 848.3184465698365), 'acceleration': (15.541326530612228, 2.7553429127509963)}\n",
            "entries in one_hot field {'cylinders': [3.0, 4.0, 5.0, 6.0, 8.0], 'origin': [1.0, 2.0, 3.0]}\n",
            "1 <function perceptron at 0x7fc41b8ce510> 0.7908333333333333\n",
            "avg and std {'displacement': (388.3482142857143, 302.0458123396403), 'horsepower': (509.3545918367347, 333.6521151716361), 'weight': (2977.5841836734694, 848.3184465698365), 'acceleration': (15.541326530612228, 2.7553429127509963)}\n",
            "entries in one_hot field {'cylinders': [3.0, 4.0, 5.0, 6.0, 8.0], 'origin': [1.0, 2.0, 3.0]}\n",
            "1 <function averaged_perceptron at 0x7fc41b8ce730> 0.9004487179487182\n",
            "avg and std {'displacement': (388.3482142857143, 302.0458123396403), 'horsepower': (509.3545918367347, 333.6521151716361), 'weight': (2977.5841836734694, 848.3184465698365), 'acceleration': (15.541326530612228, 2.7553429127509963)}\n",
            "entries in one_hot field {'cylinders': [3.0, 4.0, 5.0, 6.0, 8.0], 'origin': [1.0, 2.0, 3.0]}\n",
            "10 <function perceptron at 0x7fc41b8ce510> 0.8061538461538461\n",
            "avg and std {'displacement': (388.3482142857143, 302.0458123396403), 'horsepower': (509.3545918367347, 333.6521151716361), 'weight': (2977.5841836734694, 848.3184465698365), 'acceleration': (15.541326530612228, 2.7553429127509963)}\n",
            "entries in one_hot field {'cylinders': [3.0, 4.0, 5.0, 6.0, 8.0], 'origin': [1.0, 2.0, 3.0]}\n",
            "10 <function averaged_perceptron at 0x7fc41b8ce730> 0.8979487179487181\n",
            "avg and std {'displacement': (388.3482142857143, 302.0458123396403), 'horsepower': (509.3545918367347, 333.6521151716361), 'weight': (2977.5841836734694, 848.3184465698365), 'acceleration': (15.541326530612228, 2.7553429127509963)}\n",
            "entries in one_hot field {'cylinders': [3.0, 4.0, 5.0, 6.0, 8.0], 'origin': [1.0, 2.0, 3.0]}\n",
            "50 <function perceptron at 0x7fc41b8ce510> 0.8060256410256409\n",
            "avg and std {'displacement': (388.3482142857143, 302.0458123396403), 'horsepower': (509.3545918367347, 333.6521151716361), 'weight': (2977.5841836734694, 848.3184465698365), 'acceleration': (15.541326530612228, 2.7553429127509963)}\n",
            "entries in one_hot field {'cylinders': [3.0, 4.0, 5.0, 6.0, 8.0], 'origin': [1.0, 2.0, 3.0]}\n",
            "50 <function averaged_perceptron at 0x7fc41b8ce730> 0.9005128205128207\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "llPTEhlnpHLo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "outputId": "b461806f-9aab-48c9-e8ed-3056247c9e4d"
      },
      "source": [
        "\n",
        "auto_data, auto_labels = hw3.auto_data_and_labels(auto_data_all, features2)\n",
        "averaged_perceptron(auto_data, auto_labels, {\"T\":1})"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "avg and std {'displacement': (388.3482142857143, 302.0458123396403), 'horsepower': (509.3545918367347, 333.6521151716361), 'weight': (2977.5841836734694, 848.3184465698365), 'acceleration': (15.541326530612228, 2.7553429127509963)}\n",
            "entries in one_hot field {'cylinders': [3.0, 4.0, 5.0, 6.0, 8.0], 'origin': [1.0, 2.0, 3.0]}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[-0.25510204],\n",
              "        [ 0.98469388],\n",
              "        [ 0.        ],\n",
              "        [ 0.4744898 ],\n",
              "        [-0.52040816],\n",
              "        [ 0.0977406 ],\n",
              "        [-0.89763413],\n",
              "        [-1.84711922],\n",
              "        [ 0.06270238],\n",
              "        [-0.04591837],\n",
              "        [ 0.98469388],\n",
              "        [-0.25510204]]), array([[0.68367347]]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wm5Nu8izrGr_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sh-D5bvTbizf",
        "colab_type": "text"
      },
      "source": [
        "## 5.2) Food Reviews - Evaluating Algorithmic and Feature Choices\n",
        "\n",
        "We have supplied you with the `load_review_data`\n",
        "function, that can be used to read a .tsv file and return the labels\n",
        "and texts. We have also supplied you with the `bag_of_words` function,\n",
        "which takes the raw data and returns a dictionary of unigram\n",
        "words. The resulting dictionary is an input to\n",
        "`extract_bow_feature_vectors` which computes a feature matrix of ones\n",
        "and zeros that can be used as the input for the classification\n",
        "algorithms.  The file `hw3_part2_main.py` has code for constructing\n",
        "the data and label arrays.  Using these arrays and our implementation\n",
        "of the learning algorithms, you will be able to compute $\\theta$ and\n",
        "$\\theta_0$.  You will need to add the\n",
        "implementation of Perceptron and Averaged Perceptron."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5gHVvGl2bsps",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2c9a5676-2a94-47fe-c91c-c6bb242476b9"
      },
      "source": [
        "# Returns lists of dictionaries.  Keys are the column names, 'sentiment' and 'text'.\n",
        "# The train data has 10,000 examples\n",
        "review_data = hw3.load_review_data('reviews.tsv')\n",
        "\n",
        "# Lists texts of reviews and list of labels (1 or -1)\n",
        "review_texts, review_label_list = zip(*((sample['text'], sample['sentiment']) for sample in review_data))\n",
        "\n",
        "# The dictionary of all the words for \"bag of words\"\n",
        "dictionary = hw3.bag_of_words(review_texts)\n",
        "\n",
        "# The standard data arrays for the bag of words\n",
        "review_bow_data = hw3.extract_bow_feature_vectors(review_texts, dictionary)\n",
        "review_labels = hw3.rv(review_label_list)\n",
        "print('review_bow_data and labels shape', review_bow_data.shape, review_labels.shape)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "review_bow_data and labels shape (19945, 10000) (1, 10000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wR9S-6DsdDFF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 440
        },
        "outputId": "61195a10-b78e-4a48-e9cb-6e38e1adfc32"
      },
      "source": [
        "# Your code here to process the food review data\n",
        "for tval in [1,10,50]:\n",
        "  for learner in [perceptron, averaged_perceptron]:\n",
        "    print (tval, learner, xval_learning_alg(lambda review_bow_data, review_labels : learner(review_bow_data, review_labels, {\"T\": tval}), review_bow_data, review_labels, k=10))\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1 <function perceptron at 0x7fc41b8ce510> 0.7672000000000001\n",
            "1 <function averaged_perceptron at 0x7fc41b8ce730> 0.8120999999999998\n",
            "10 <function perceptron at 0x7fc41b8ce510> 0.7871\n",
            "10 <function averaged_perceptron at 0x7fc41b8ce730> 0.8237\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-f501bb656240>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mlearner\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mperceptron\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maveraged_perceptron\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxval_learning_alg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mreview_bow_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreview_labels\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mlearner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreview_bow_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreview_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"T\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtval\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreview_bow_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreview_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-3-7ea2467a4ca4>\u001b[0m in \u001b[0;36mxval_learning_alg\u001b[0;34m(learner, data, labels, k)\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0mlabels_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         score_sum += eval_classifier(learner, data_train, labels_train,\n\u001b[0;32m---> 67\u001b[0;31m                                               data_test, labels_test)\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mscore_sum\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-7ea2467a4ca4>\u001b[0m in \u001b[0;36meval_classifier\u001b[0;34m(learner, data_train, labels_train, data_test, labels_test)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0meval_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m     \u001b[0mth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mth0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlearner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mth0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mdata_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-f501bb656240>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(review_bow_data, review_labels)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mlearner\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mperceptron\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maveraged_perceptron\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxval_learning_alg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mreview_bow_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreview_labels\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mlearner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreview_bow_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreview_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"T\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtval\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreview_bow_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreview_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-3-7ea2467a4ca4>\u001b[0m in \u001b[0;36mperceptron\u001b[0;34m(data, labels, params, hook)\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mpositive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta_0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m                 \u001b[0mmistakes\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0mtheta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtheta\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-7ea2467a4ca4>\u001b[0m in \u001b[0;36mpositive\u001b[0;34m(x, th, th0)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpositive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mth0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m@\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mth0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mth0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xHpIJ5-46M6W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "theta, theta0 = averaged_perceptron(review_bow_data,review_labels,{\"T\":10})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i8d31wW66tAD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "93bac2aa-f75a-4cf1-9adb-34077cf702e4"
      },
      "source": [
        "ind = np.argsort(theta.T)\n",
        "print (theta.shape, ind[0, 0:10], ind[0,-10:])\n",
        "print ([hw3.reverse_dict(dictionary)[key] for key in ind[0,0:10]])\n",
        "print ([hw3.reverse_dict(dictionary)[key] for key in ind[0,-10:]])"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(19945, 1) [ 305  492 2352 2003 2633 3126 2877 2571 1406 2424] [  56 2213 2329 1476 2413  356 1896 1105  348 1041]\n",
            "['worst', 'awful', 'unfortunately', 'horrible', 'stuck', 'changed', 'disappointment', 'bland', 'poor', 'formula']\n",
            "['great', 'individually', 'bright', 'yummy', 'skeptical', 'perfect', 'easily', 'satisfied', 'delicious', 'excellent']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qbqHMbrubt5t",
        "colab_type": "text"
      },
      "source": [
        "## 6.2) Evaluating Features for MNIST Data\n",
        "\n",
        "\n",
        "This problem explores how well the perceptron algorithm works to <a\n",
        "href=\"http://neuralnetworksanddeeplearning.com/chap1.html\">classify\n",
        "images of handwritten digits</a>, from the well-known (\"MNIST\")\n",
        "dataset, buiding on your thoughts from lab about extracting features\n",
        "from images.  This exercise will highlight how important feature\n",
        "extraction is, before linear classification is done, using algorithms\n",
        "such as the Perceptron.\n",
        "\n",
        "<b>Dataset setup</b>\n",
        "\n",
        "Often, it may be easier to work with a vector whose spatial orientation is preserved.\n",
        "In previous parts, we have represented features as one long feature vector.\n",
        "For images, however, we often represent a $m$ by $n$ image\n",
        "as a `(m,n)` array, rather than a `(mn,1)` array\n",
        "(as the previous parts have done).\n",
        "\n",
        "In the code file, we have supplied you with the `load_mnist_data` function,\n",
        "which will read from the provided image files and populate a dictionary,\n",
        "with image and label vectors for each numerical digit from 0 to 9.\n",
        "These images are already shaped as `(m,n)` arrays."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_6xT_UA2cJMe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c26c559b-d773-429d-b6ae-0ba1d69e653b"
      },
      "source": [
        "mnist_data_all = hw3.load_mnist_data(range(10))\n",
        "\n",
        "print('mnist_data_all loaded. shape of single images is', mnist_data_all[0][\"images\"][0].shape)\n",
        "\n",
        "# HINT: change the [0] and [1] if you want to access different images\n",
        "d0 = mnist_data_all[0][\"images\"]\n",
        "d1 = mnist_data_all[1][\"images\"]\n",
        "y0 = np.repeat(-1, len(d0)).reshape(1,-1)\n",
        "y1 = np.repeat(1, len(d1)).reshape(1,-1)\n",
        "\n",
        "# data goes into the feature computation functions\n",
        "data = np.vstack((d0, d1))\n",
        "# labels can directly go into the Perceptron algorithm\n",
        "labels = np.vstack((y0.T, y1.T)).T"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mnist_data_all loaded. shape of single images is (28, 28)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vsX_9X7Nb0NW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# change these implementations to support whole datasets\n",
        "\n",
        "def raw_mnist_features(x):\n",
        "    \"\"\"\n",
        "    @param x (n_samples,m,n) array with values in (0,1)\n",
        "    @return (m*n,n_samples) reshaped array where each entry is preserved\n",
        "    \"\"\"\n",
        "    n_samples, m,n = x.shape\n",
        "    return np.transpose(x.reshape(n_samples, m*n))\n",
        "    raise Exception(\"implement me!\")\n",
        "\n",
        "def row_average_features(x):\n",
        "    \"\"\"\n",
        "    This should either use or modify your code from the tutor questions.\n",
        "\n",
        "    @param x (n_samples,m,n) array with values in (0,1)\n",
        "    @return (m,n_samples) array where each entry is the average of a row\n",
        "    \"\"\"\n",
        "    return np.transpose(np.mean(x,axis=2, keepdims=False))\n",
        "    raise Exception(\"modify me!\")\n",
        "\n",
        "\n",
        "def col_average_features(x):\n",
        "    \"\"\"\n",
        "    This should either use or modify your code from the tutor questions.\n",
        "\n",
        "    @param x (n_samples,m,n) array with values in (0,1)\n",
        "    @return (n,n_samples) array where each entry is the average of a column\n",
        "    \"\"\"\n",
        "    return np.mean(x, axis=1, keepdims=False).T\n",
        "    raise Exception(\"modify me!\")\n",
        "\n",
        "\n",
        "def top_bottom_features(x):\n",
        "    \"\"\"\n",
        "    This should either use or modify your code from the tutor questions.\n",
        "\n",
        "    @param x (n_samples,m,n) array with values in (0,1)\n",
        "    @return (2,n_samples) array where the first entry of each column is the average of the\n",
        "    top half of the image = rows 0 to floor(m/2) [exclusive]\n",
        "    and the second entry is the average of the bottom half of the image\n",
        "    = rows floor(m/2) [inclusive] to m\n",
        "    \"\"\"\n",
        "    n_samples, rows, columns = x.shape\n",
        "    top_half = x[:, 0:rows//2,:]\n",
        "    bottom_half = x[:,rows//2:,:]\n",
        "    return np.array([np.mean(top_half, axis = (1,2)), np.mean(bottom_half, axis = (1,2))])\n",
        "    raise Exception(\"modify me!\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9DTXfMoDgCKk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "ebf06ee3-2f6f-43fc-a407-ed051b853a8b"
      },
      "source": [
        "# use this function to evaluate accuracy\n",
        "\n",
        "for digit in [(0,1), (2,4), (6,8), (9,0)]:\n",
        "\n",
        "  d0 = mnist_data_all[digit[0]][\"images\"]\n",
        "  d1 = mnist_data_all[digit[1]][\"images\"]\n",
        "  y0 = np.repeat(-1, len(d0)).reshape(1,-1)\n",
        "  y1 = np.repeat(1, len(d1)).reshape(1,-1)\n",
        "\n",
        "  # data goes into the feature computation functions\n",
        "  data = np.vstack((d0, d1))\n",
        "  # labels can directly go into the Perceptron algorithm\n",
        "  labels = np.vstack((y0.T, y1.T)).T\n",
        "  acc = hw3.get_classification_accuracy(raw_mnist_features(data), labels)\n",
        "  print (digit, acc)"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(0, 1) 0.975\n",
            "(2, 4) 0.8641666666666665\n",
            "(6, 8) 0.9479166666666667\n",
            "(9, 0) 0.6470833333333333\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F0PVd51rN3JY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "outputId": "87e08269-29d0-4a3f-e48e-f2bcd35efcde"
      },
      "source": [
        "for digit in [(0,1), (2,4), (6,8), (9,0)]:\n",
        "  for features in [row_average_features, col_average_features, top_bottom_features]:\n",
        "  \n",
        "    d0 = mnist_data_all[digit[0]][\"images\"]\n",
        "    d1 = mnist_data_all[digit[1]][\"images\"]\n",
        "    y0 = np.repeat(-1, len(d0)).reshape(1,-1)\n",
        "    y1 = np.repeat(1, len(d1)).reshape(1,-1)\n",
        "\n",
        "    # data goes into the feature computation functions\n",
        "    data = np.vstack((d0, d1))\n",
        "    # labels can directly go into the Perceptron algorithm\n",
        "    labels = np.vstack((y0.T, y1.T)).T\n",
        "    acc = hw3.get_classification_accuracy(features(data), labels)\n",
        "    print (digit, features, acc)"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(0, 1) <function row_average_features at 0x7fc41a1a70d0> 0.48125\n",
            "(0, 1) <function col_average_features at 0x7fc4178c7ea0> 0.6375\n",
            "(0, 1) <function top_bottom_features at 0x7fc4178c7598> 0.48125\n",
            "(2, 4) <function row_average_features at 0x7fc41a1a70d0> 0.7754166666666668\n",
            "(2, 4) <function col_average_features at 0x7fc4178c7ea0> 0.49749999999999994\n",
            "(2, 4) <function top_bottom_features at 0x7fc4178c7598> 0.49749999999999994\n",
            "(6, 8) <function row_average_features at 0x7fc41a1a70d0> 0.92125\n",
            "(6, 8) <function col_average_features at 0x7fc4178c7ea0> 0.52125\n",
            "(6, 8) <function top_bottom_features at 0x7fc4178c7598> 0.5650000000000001\n",
            "(9, 0) <function row_average_features at 0x7fc41a1a70d0> 0.49749999999999994\n",
            "(9, 0) <function col_average_features at 0x7fc4178c7ea0> 0.5041666666666667\n",
            "(9, 0) <function top_bottom_features at 0x7fc4178c7598> 0.49749999999999994\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}